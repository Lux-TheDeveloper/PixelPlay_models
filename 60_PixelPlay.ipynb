{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90860,"databundleVersionId":10652987,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nfrom glob import glob\nfrom PIL import Image\nimport torch\nfrom torch.utils import data\nimport torchvision.transforms as transforms\nimport numpy as n\nimport torch.nn as nn\nimport torch.optim as optim\nimport argparse\nimport sys\nimport csv\nfrom sklearn.preprocessing import normalize\nimport pandas as pd\nimport IPython.display as ipd\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models, datasets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataset(data.Dataset):\n    def __init__(self, image_dir, classes_file, predicate_file, transform=None):\n        self.transform = transform\n        self.predicate_continuous_mat = np.array(np.genfromtxt(predicate_file, dtype='float32'))\n\n      # i am normalizing only positive values , not negative as negative is -1 and i dont want my model to ever get its features\n        self.predicate_continuous_mat[self.predicate_continuous_mat > 0] /= 100.0 \n\n        #matches names to index (index starts from 0)\n        self.class_to_index = {}\n        with open(classes_file) as f:\n            for line in f:\n                index, class_name = line.split()\n                self.class_to_index[class_name.strip()] = int(index) - 1  \n\n        self.img_names = []\n        self.img_index = []\n        for class_name, class_index in self.class_to_index.items():\n            folder_dir = os.path.join(image_dir, class_name)\n            file_descriptor = os.path.join(folder_dir, '*.jpg')\n            files = glob(file_descriptor)\n            for file_name in files:\n                self.img_names.append(file_name)\n                self.img_index.append(class_index)\n\n        print(f\"Loaded {len(self.img_names)} images from {image_dir}\")\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, index):\n        img_path = self.img_names[index]\n        image = Image.open(img_path)\n        if image.getbands()[0] == 'L':  \n            image = image.convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        img_index = self.img_index[index]\n        predicate = self.predicate_continuous_mat[img_index, :]\n        return image, predicate, img_path, img_index\n\n#class defined for first making image to square by padding and then resizing it as if resied rectangle images, data will be lost\nclass PadToSquare:\n    def __call__(self, image):\n        width, height = image.size\n        max_side = max(width, height)\n        padding = (\n            (max_side - width) // 2, \n            (max_side - height) // 2, \n            (max_side - width + 1) // 2, \n            (max_side - height + 1) // 2\n        )\n        return transforms.functional.pad(image, padding, fill=0, padding_mode='constant')\n\nclass CustomResNet(nn.Module):\n    def __init__(self, num_predicates, num_classes):\n        super(CustomResNet, self).__init__()\n        self.resnet = torchvision.models.resnet50(pretrained=True)#loads pre trained resnet50 model\n        \n       \n        self.resnet.fc = nn.Identity()# this line removes FC layer \n        \n        #these lines adds 2 new layers , one for the prediactes and other for the classes \n        self.fc_predicates = nn.Linear(2048, num_predicates)\n        self.fc_classes = nn.Linear(num_predicates, num_classes)\n    \n    def forward(self, x):\n        x = self.resnet(x)\n        predicates = self.fc_predicates(x)\n        class_logits = self.fc_classes(predicates)\n        return class_logits, predicates\n\n\n\n\nnum_predicates = 85\nnum_classes = 50\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CustomResNet(num_predicates=num_predicates, num_classes=num_classes).to(device) # shifts to gpu\n\n\n\n\n# cross entropy with label smoothing\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        self.smoothing = smoothing\n        self.confidence = 1.0 - smoothing\n\n    def forward(self, x, target):\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n\n\n\n\n#vse loss function\nclass VSELoss(nn.Module):\n    def __init__(self, scale_factor=1.0):\n        super(VSELoss, self).__init__()\n        self.scale_factor = scale_factor\n\n    def forward(self, y_pred, y_true):\n        return torch.mean((y_pred - y_true) ** 2 * (1 + self.scale_factor * (y_pred - y_true) ** 2))\n\n\n\n\ncriterion_predicates = VSELoss(scale_factor=1.0)  \ncriterion_classes = LabelSmoothingCrossEntropy(smoothing=0.1) \n\n\n# i have defined this train class to modify my code to make finetuning easier and keeping all parameters at one place , using parsed arguments to calltrain function \ndef train(num_epochs, eval_interval, learning_rate, output_filename, batch_size):\n    train_params = {'batch_size': batch_size, 'num_workers': 3}\n    val_params = {'batch_size': batch_size, 'num_workers': 3}\n    train_process_steps = transforms.Compose([\n        PadToSquare(),\n        transforms.RandomRotation(15),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n        transforms.Resize((224, 224)),  \n        transforms.ToTensor()\n    ])##torchvision transformations \n    dataset = CustomDataset('/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/train', '/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/classes.txt', '/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/predicate-matrix-continuous.txt', train_process_steps)\n    \n\n    #nothing , just dividing my dataset into train and validation set and defining loaders\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(0.05 * dataset_size)) \n    np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n\n    train_sampler = data.SubsetRandomSampler(train_indices)\n    val_sampler = data.SubsetRandomSampler(val_indices)\n\n    train_loader = data.DataLoader(dataset, sampler=train_sampler, **train_params)\n    val_loader = data.DataLoader(dataset, sampler=val_sampler, **val_params)\n\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  #using adamW optimizer\n\n   \n    \n    \n    \n    #training loop starts \n    for epoch in range(num_epochs):\n        model.train()\n        for i, (images, predicates, img_names, indexes) in enumerate(train_loader):\n            if images.shape[0] < 2:\n                break\n            images = images.to(device)\n            predicates = predicates.to(device)\n            \n            #forwardpass\n            class_logits, pred_predicates = model(images)\n            class_targets = indexes.to(device)  # Assuming `indexes` represent class labels\n\n            loss_predicates = criterion_predicates(pred_predicates, predicates)\n            loss_classes = criterion_classes(class_logits, class_targets)\n            loss = 0.5 * loss_predicates + 0.5 * loss_classes  ## the place where i managed weights of loss\n\n           #doing backward pass and optimizing , normal things\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if (i+1) % eval_interval == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}, Predicate Loss: {loss_predicates.item():.4f}, Class Loss: {loss_classes.item():.4f}\")\n        \n        #evaluating on validation set \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for images, predicates, img_names, indexes in val_loader:\n                images = images.to(device)\n                predicates = predicates.to(device)\n                class_logits, pred_predicates = model(images)\n                class_targets = indexes.to(device)\n                \n                \n                loss_predicates = criterion_predicates(pred_predicates, predicates)\n                loss_classes = criterion_classes(class_logits, class_targets)\n                loss = 0.5 * loss_predicates + 0.5 * loss_classes  ## the place where i managed weights of loss (same changes done on validtion, otherwise nothing can be concluded from loss trend)\n\n                val_loss += loss.item()\n        \n        val_loss /= len(val_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}\")\n    \n    \n    torch.save(model.state_dict(), output_filename) ##saving trained model \n\n\n# i know that typically parse statements should not be used in python notebooks,it was an experiment can i use this efficiently in ipynb files \n#another reason was that i want all my hyperparameters to be placed at same place , so that nothing gets left unchanged and my traning dont gets meaningless if any of the change is left \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Train a model with CustomResNet and combined loss')\n    parser.add_argument('--num_epochs', type=int, default=10, help='Number of training epochs')\n    parser.add_argument('--eval_interval', type=int, default=100, help='Evaluation interval')\n    parser.add_argument('--learning_rate', type=float, default=0.00005, help='Learning rate')  \n    parser.add_argument('--output_filename', type=str, default='model.pth', help='Output filename for the trained model')\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n\n    args, unknown = parser.parse_known_args()\n\n    train(args.num_epochs, args.eval_interval, args.learning_rate, args.output_filename, args.batch_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this is training cell\n\n# should be defined at starting by convetion but as i am not distributing in cells , it wont be neat even if i put above \ntest_transform = transforms.Compose([\n    PadToSquare(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n\nclass_to_index = {}\nwith open('/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/classes.txt') as f:\n    for line in f:\n        index, class_name = line.split()\n        class_to_index[int(index) - 1] = class_name.strip()\n\n\ndef get_modified_weights(model):### saves modified weights \n    return model.fc_classes.weight.detach().cpu().numpy()\n\n#this function is made to calculate the closest class\ndef find_closest_class(model, image, unseen_weights, seen_weights):\n    with torch.no_grad():\n        image = test_transform(image).unsqueeze(0).to(device)\n        class_logits, predicates = model(image)\n        predicates = predicates.cpu().numpy().flatten()\n        \n        # here normalization of predicates is done alligned with weights\n        predicates = normalize(predicates.reshape(1, -1), norm='l2')[0]\n\n    # these lines calculate cosine similarities with all the classes(both seen and unseen classes)\n    seen_similarity = np.dot(seen_weights, predicates)\n    unseen_similarity = np.dot(unseen_weights, predicates)\n    \n    ## exact lines where closest class is found\n    if np.max(seen_similarity) > np.max(unseen_similarity):\n        return np.argmax(seen_similarity)\n    else:\n        return 40 + np.argmax(unseen_similarity)  # Assuming unseen classes indices start from 40\n\n\n\nmodified_weights = get_modified_weights(model)\n\n\npredicate_continuous_mat = np.array(np.genfromtxt('/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/predicate-matrix-continuous.txt', dtype='float32'))\npredicate_continuous_mat[predicate_continuous_mat > 0] /= 100.0 # normalizes P-M-C\n\n\nseen_weights = modified_weights[:40]\nunseen_weights = predicate_continuous_mat[40:]\n\n# normalizes the weights \nseen_weights = normalize(seen_weights, norm='l2')\nunseen_weights = normalize(unseen_weights, norm='l2')\n\n# predictions are stored in this file \npredictions = []\ntest_image_dir = '/kaggle/input/vlg-recruitment-24-challenge/vlg-dataset/test'\n\nfor test_image_path in glob(os.path.join(test_image_dir, '*.jpg')):\n    test_image = Image.open(test_image_path).convert('RGB')\n    label_index = find_closest_class(model, test_image, unseen_weights, seen_weights)\n    animal_name = class_to_index.get(label_index, \"Unknown\")\n    image_id = os.path.basename(test_image_path)\n    predictions.append((image_id, animal_name))\n\n# making csv file for the outputs \ncsv_filename = 'predictions.csv'\nwith open(csv_filename, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['image_id', 'class']) \n\n    for image_id, animal_name in predictions:\n        writer.writerow([image_id, animal_name])\n\nprint(f\"CSV file '{csv_filename}' created successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n## code for sorting csv file and creating a download link \n\ndf = pd.read_csv('predictions.csv')\ndf_sorted = df.sort_values(by='image_id')\nsorted_csv_filename = 'sorted_predictions.csv'\ndf_sorted.to_csv(sorted_csv_filename, index=False)\n\nprint(f\"Sorted CSV file '{sorted_csv_filename}' created successfully.\")\n\nfile_path = sorted_csv_filename\nassert os.path.isfile(file_path), f\"{file_path} does not exist.\"\ndownload_link = f'<a href=\"{file_path}\" download>Download sorted_predictions.csv</a>'\nipd.display(ipd.HTML(download_link))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}